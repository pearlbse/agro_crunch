{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from unidecode import unidecode\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "iso_df = pd.read_csv('input/isocodes.csv')  \n",
    "codes_original = pd.read_json('input/geonames.json')\n",
    "\n",
    "# nan_counts_pc = []\n",
    "# nan_counts_l = []\n",
    "\n",
    "# for index, row in iso_df.iterrows():\n",
    "\n",
    "iso2 = 'CN'\n",
    "iso3 = 'CHN'\n",
    "\n",
    "# Crunchbase Dataframe\n",
    "df = pd.read_csv('input/foodtech.csv')\n",
    "df = df[df['country_code'] == iso3]\n",
    "df = df[['uuid','country_code', 'state_code', 'region', 'city', 'address', 'postal_code']]\n",
    "\n",
    "df['city'] = df['city'].apply(unidecode)\n",
    "df['region'] = df['region'].apply(unidecode)\n",
    "\n",
    "df['city'] = df['city'].str.lower()\n",
    "df['region'] = df['region'].str.lower()\n",
    "\n",
    "# Postal Codes\n",
    "codes = codes_original.copy()     \n",
    "codes = codes[codes['country_code'] == iso2]\n",
    "\n",
    "codes['admin_name1'] = codes['admin_name1'].apply(lambda x: unidecode(x) if x is not None else None)\n",
    "codes['admin_name2'] = codes['admin_name2'].apply(lambda x: unidecode(x) if x is not None else None)\n",
    "codes['place_name'] = codes['place_name'].apply(lambda x: unidecode(x) if x is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.read_excel('input/foodtech_legacy_vasiliki.xlsx')\n",
    "ref = ref[ref['ISO'] == iso3]\n",
    "\n",
    "ref = ref[['City', 'New name ', 'Country.1',  'LONG', 'LAT']]\n",
    "ref.columns = map(str.lower, ref.columns)\n",
    "ref.drop_duplicates(subset='city', inplace=True)\n",
    "\n",
    "df = pd.merge(df, ref, how='left', on='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. EXACT MATCHES ####\n",
    "\n",
    "# `place_name`\n",
    "codes1 = codes.copy()\n",
    "codes1 = codes1[['postal_code', 'latitude', 'longitude', 'admin_name1', 'place_name']]\n",
    "\n",
    "codes1['admin_name1'] = codes1['admin_name1'].str.lower()\n",
    "codes1['place_name'] = codes1['place_name'].str.lower()\n",
    "\n",
    "codes1.drop_duplicates(subset=['admin_name1', 'place_name'], inplace=True)\n",
    "\n",
    "merged_df = pd.merge(df, codes1, left_on=['city', 'region'], right_on=['place_name', 'admin_name1'], how='left')\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n",
    "# `admin_name2`\n",
    "codes2 = codes.copy()\n",
    "codes2 = codes2[['postal_code', 'latitude', 'longitude', 'admin_name1', 'admin_name2']]\n",
    "\n",
    "codes2['admin_name1'] = codes2['admin_name1'].str.lower()\n",
    "codes2['admin_name2'] = codes2['admin_name2'].str.lower()\n",
    "\n",
    "codes2.drop_duplicates(subset=['admin_name1', 'admin_name2'], inplace=True)\n",
    "\n",
    "merged_df = pd.merge(merged_df, codes2, left_on=['city', 'region'], right_on=['admin_name2', 'admin_name1'], how='left')\n",
    "# merged_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "\n",
    "merged_df['postal_code_y'].fillna(merged_df['postal_code'], inplace=True)\n",
    "\n",
    "merged_df['latitude_x'].fillna(merged_df['latitude_y'], inplace=True)\n",
    "merged_df['longitude_x'].fillna(merged_df['longitude_y'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n",
    "# `admin_name3`\n",
    "codes3 = codes.copy()\n",
    "codes3 = codes3[['postal_code','admin_name1', 'admin_name3', 'latitude', 'longitude']]\n",
    "\n",
    "codes3['admin_name1'] = codes3['admin_name1'].str.lower()\n",
    "codes3['admin_name3'] = codes3['admin_name3'].str.lower()\n",
    "\n",
    "codes3.drop_duplicates(subset=['admin_name1', 'admin_name3'], inplace=True)\n",
    "\n",
    "merged_df = pd.merge(merged_df, codes3, left_on=['city', 'region'], right_on=['admin_name3', 'admin_name1'], how='left')\n",
    "\n",
    "merged_df['postal_code_y'].fillna(merged_df['postal_code'], inplace=True)\n",
    "\n",
    "merged_df['latitude_x'].fillna(merged_df['latitude'], inplace=True)\n",
    "merged_df['longitude_x'].fillna(merged_df['longitude'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n",
    "# `place_name` ONLY\n",
    "codes1.drop_duplicates(subset=['place_name'], inplace=True)\n",
    "merged_df = pd.merge(merged_df, codes1, left_on=['city'], right_on=['place_name'], how='left')\n",
    "\n",
    "merged_df['postal_code_y'].fillna(merged_df['postal_code'], inplace=True)\n",
    "\n",
    "merged_df['latitude_x'].fillna(merged_df['latitude'], inplace=True)\n",
    "merged_df['longitude_x'].fillna(merged_df['longitude'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3187 entries, 0 to 3186\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   uuid           3187 non-null   object \n",
      " 1   country_code   3187 non-null   object \n",
      " 2   state_code     0 non-null      object \n",
      " 3   region         3187 non-null   object \n",
      " 4   city           3187 non-null   object \n",
      " 5   address        1503 non-null   object \n",
      " 6   postal_code_x  741 non-null    object \n",
      " 7   postal_code_y  2358 non-null   object \n",
      " 8   latitude_x     2358 non-null   float64\n",
      " 9   longitude_x    2358 non-null   float64\n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 273.9+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. EXISTING POSTAL CODES: FUZZY MATCHES ####\n",
    "\n",
    "# # Filter out rows where both 'postal_code_x' and 'postal_code_y' are not NaN\n",
    "# filtered_df = merged_df[(merged_df['postal_code_x'].notna()) & (merged_df['postal_code_y'].isna())]\n",
    "\n",
    "# # Calculate the desired length based on the length of postal codes in codes['postal_code']\n",
    "# desired_length = len(codes['postal_code'].iloc[0])\n",
    "\n",
    "# # Create a new column in filtered_df to truncate 'postal_code_x' values\n",
    "# filtered_df['postal_code_x_truncated'] = filtered_df['postal_code_x'].str.slice(0, desired_length)\n",
    "\n",
    "# # Define a function for fuzzy matching and selecting the best match\n",
    "# def fuzzy_select_best_match(row_value, choices):\n",
    "#     # Ensure row_value is a string\n",
    "#     row_value = str(row_value)\n",
    "#     best_match = process.extractOne(row_value, choices, scorer=fuzz.token_sort_ratio)\n",
    "#     return best_match[0], best_match[1]\n",
    "\n",
    "# # Apply fuzzy matching and select best match for each value in 'postal_code_x_truncated' in the filtered_df\n",
    "# filtered_df['fuzzy_match'], filtered_df['fuzzy_score'] = zip(*filtered_df['postal_code_x_truncated'].apply(lambda x: fuzzy_select_best_match(x, codes['postal_code'])))\n",
    "# filtered_df = filtered_df[filtered_df['fuzzy_score'] > 50]\n",
    "\n",
    "# filtered_df = pd.merge(filtered_df, codes[['postal_code', 'latitude', 'longitude']], how='left', left_on='fuzzy_match', right_on='postal_code', suffixes=('', '_codes'))\n",
    "# filtered_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "\n",
    "# merged_df = pd.merge(merged_df, filtered_df[['uuid','postal_code', 'latitude', 'longitude']], how='left', on='uuid')\n",
    "\n",
    "# merged_df['postal_code_y'].fillna(merged_df['postal_code'], inplace=True)\n",
    "# merged_df['latitude_x'].fillna(merged_df['latitude'], inplace=True)\n",
    "# merged_df['longitude_x'].fillna(merged_df['longitude'], inplace=True)\n",
    "\n",
    "# merged_df = merged_df.iloc[:, 0:10] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['postal_code_y'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_df is empty. Skipping substring matching and subsequent tasks.\n"
     ]
    }
   ],
   "source": [
    "#### 3. PARTIAL MATCHES ####\n",
    "\n",
    "# Define the columns for substring containment\n",
    "left_on = 'city'\n",
    "right_on = {\n",
    "    'codes1': 'place_name',\n",
    "    'codes2': 'admin_name2',\n",
    "    'codes3': 'admin_name3'\n",
    "}\n",
    "\n",
    "# Define a function for substring matching and selecting the best match\n",
    "def substring_select_best_match(row, dfs, left_col, right_cols):\n",
    "    best_match = None\n",
    "    best_postal_code = None\n",
    "    best_lat = None\n",
    "    best_long = None\n",
    "    \n",
    "    for df_name, right_col in right_cols.items():\n",
    "        # Filter out NaN values before applying str.contains\n",
    "        matches = dfs[df_name][right_col].str.contains(row[left_col], case=False, na=False)\n",
    "        if any(matches):\n",
    "            best_match = dfs[df_name][right_col][matches].iloc[0]\n",
    "            best_postal_code = dfs[df_name].loc[matches, 'postal_code'].iloc[0]\n",
    "            best_lat = dfs[df_name].loc[matches, 'latitude'].iloc[0]\n",
    "            best_long = dfs[df_name].loc[matches, 'longitude'].iloc[0]\n",
    "            break  # Exit loop after finding a match\n",
    "    \n",
    "    return best_match, best_postal_code, 100, best_lat, best_long  # Always return best_score as 100\n",
    "\n",
    "# Filter rows where postal_code_y is NaN\n",
    "filtered_df = merged_df[merged_df['postal_code_y'].isna()]\n",
    "\n",
    "if not filtered_df.empty:  # Check if filtered_df is not empty\n",
    "    # Apply substring matching and select best match for each row in filtered_df\n",
    "    matches_df = filtered_df.apply(lambda row: substring_select_best_match(row, {'codes1': codes1, 'codes2': codes2, 'codes3': codes3}, left_on, right_on), axis=1, result_type='expand')\n",
    "    matches_df.columns = ['best_match', 'best_postal_code', 'best_score', 'best_lat', 'best_long']\n",
    "\n",
    "    matches_df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "    merged_df = pd.merge(merged_df, matches_df, how='left', left_index=True, right_index=True)\n",
    "    merged_df['postal_code_y'].fillna(merged_df['best_postal_code'], inplace=True)\n",
    "    merged_df['latitude_x'].fillna(merged_df['best_lat'], inplace=True)\n",
    "    merged_df['longitude_x'].fillna(merged_df['best_long'], inplace=True)\n",
    "\n",
    "    merged_df = merged_df.iloc[:, 0:10]\n",
    "else:\n",
    "    print(\"filtered_df is empty. Skipping substring matching and subsequent tasks.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. EXISTING POSTAL CODES ####\n",
    "\n",
    "# Across rows: df\n",
    "codes_df = merged_df[merged_df['postal_code_y'].notna()]\n",
    "\n",
    "codes_df = codes_df[['region', 'city', 'postal_code_y', 'latitude_x', 'longitude_x']]\n",
    "codes_df.drop_duplicates(subset=['region', 'city'], inplace=True)\n",
    "\n",
    "merged_df = pd.merge(merged_df, codes_df, left_on=['city', 'region'], right_on=['city', 'region'], how='left')\n",
    "\n",
    "merged_df['postal_code_y_x'].fillna(merged_df['postal_code_y_y'], inplace=True)\n",
    "merged_df['latitude_x_x'].fillna(merged_df['latitude_x_y'], inplace=True)\n",
    "merged_df['longitude_x_x'].fillna(merged_df['longitude_x_y'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5. FILL REMAINING WITH UNPROCESSED POSTAL CODES ####\n",
    "\n",
    "# Filling NaNs with Existing Postal Codes\n",
    "merged_df['postal_code_y_x'].fillna(merged_df['postal_code_x'], inplace=True)\n",
    "\n",
    "# Across rows: df\n",
    "codes_df = df[df['postal_code'].notna()]\n",
    "\n",
    "codes_df = codes_df[['region', 'city', 'postal_code']]\n",
    "codes_df.drop_duplicates(subset=['region', 'city'], inplace=True)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, codes_df, left_on=['city', 'region'], right_on=['city', 'region'], how='left')\n",
    "merged_df['postal_code_y_x'].fillna(merged_df['postal_code'], inplace=True)\n",
    "merged_df = merged_df.iloc[:, 0:10]\n",
    "\n",
    "# Fill coordinates based on above filling\n",
    "codes4 = codes.copy()\n",
    "codes4 = codes4[['postal_code', 'latitude', 'longitude']]\n",
    "codes4.drop_duplicates(subset=['postal_code'], inplace=True)\n",
    "\n",
    "merged_df = pd.merge(merged_df, codes4, left_on=['postal_code_y_x'], right_on=['postal_code'], how='left')\n",
    "\n",
    "merged_df['latitude_x_x'].fillna(merged_df['latitude'], inplace=True)\n",
    "merged_df['longitude_x_x'].fillna(merged_df['longitude'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.iloc[:, 0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save to csv\n",
    "# merged_df = merged_df.rename(columns={'postal_code_x': 'pc_crunchbase',\n",
    "#                                         'postal_code_y_x': 'pc_filled',\n",
    "#                                         'latitude_x_x':    'latitude',\n",
    "#                                         'longitude_x_x':   'longitude'})\n",
    "\n",
    "# merged_df.to_csv('general/test/' + iso3 + '_processed_test.csv')\n",
    "\n",
    "# # Checking NaNs\n",
    "# nan_count_pc = merged_df['pc_filled'].isna().sum()\n",
    "# nan_counts_pc.append({'iso3': iso3, 'nan_count': nan_count_pc})\n",
    "\n",
    "# nan_count_l = merged_df['latitude'].isna().sum()\n",
    "# nan_counts_l.append({'iso3': iso3, 'nan_count': nan_count_l})\n",
    "\n",
    "# # Create a DataFrame from the list of dictionaries\n",
    "# nan_counts_df_pc = pd.DataFrame(nan_counts_pc)\n",
    "# nan_counts_df_l = pd.DataFrame(nan_counts_l)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
